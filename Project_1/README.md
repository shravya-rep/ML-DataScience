# ğŸ§  Vertebral Column Disorder Detection using K-Nearest Neighbors


This project applies K-Nearest Neighbors (KNN) classification on a biomedical dataset to distinguish between **normal** and **abnormal** vertebral column conditions using various distance metrics and voting strategies.

---

## ğŸ“Š Dataset Description

- **Source**: [UCI Vertebral Column Data Set](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column)
- **Features**: 6 biomechanical attributes derived from pelvis and lumbar spine shape
- **Classes**:
  - `0` â†’ Normal
  - `1` â†’ Abnormal (includes Disk Hernia, Spondylolisthesis)

---

## ğŸ”¬ Tasks Performed

### ğŸ“Œ Exploratory Data Analysis
- Scatterplots and boxplots of all independent variables colored by class label

### ğŸ“Œ Dataset Preparation
- Selected:
  - First 70 samples from Class 0
  - First 140 samples from Class 1 â†’ **Training set**
  - Remaining data â†’ **Test set**

---

## ğŸ” Classification Tasks

### 1. KNN with Euclidean Distance
- Evaluated train/test error vs. `k âˆˆ {208, 205, ..., 1}`
- Selected best `k*` based on F1-score
- Reported:
  - Confusion Matrix
  - Precision, Recall, F1-score, True Positive/Negative Rates

### 2. Learning Curve
- Varied training set size `N âˆˆ {10, 20, ..., 210}`
- For each `N`, found best `k` from `{1, 6, ..., N-4}` and plotted **test error vs. N**

---

## ğŸ“ Distance Metrics Compared

| Metric Type     | Parameter Setting |
|-----------------|-------------------|
| Euclidean       | â€“                 |
| Manhattan       | Minkowski (p = 1) |
| General p       | `log10(p) âˆˆ {0.1, ..., 1}` |
| Chebyshev       | `p â†’ âˆ`           |
| Mahalanobis     | Covariance-based  |

âœ… Best test errors from each distance function were compared in tabular form.

---

## ğŸ“Š Weighted KNN

- Implemented weighted voting where weights = `1/distance`
- Applied to:
  - Euclidean
  - Manhattan
  - Chebyshev
- Reported best test errors for each with `k âˆˆ {1, 6, ..., 196}`

---

## ğŸ Performance Summary

- **Best training error achieved**: _<insert your value here>_
- **Optimal k**: _<insert k*>_ from unweighted voting

---

## ğŸ› ï¸ Technologies Used

- Python (NumPy, pandas, matplotlib)
- scikit-learn (`neighbors.KNeighborsClassifier`, `DistanceMetric`)
- Jupyter Notebook for visualization

---

## ğŸ“Œ Future Work

- Apply PCA before distance-based learning
- Use more advanced classifiers (e.g., SVM, ensemble trees)
- Hyperparameter tuning using cross-validation
